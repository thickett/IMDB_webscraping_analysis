---
title: "IMDb_rating_predictor_Full"
author: "Callum Thickett"
date: "25/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## IMDb_rating_predictor

The goal of this personal project is to try and create a predictive model that can take information from a movie and predict what its IMDb rating would be. When looking for a movie to watch i always find my self consulting the ratings on IMDb, and i tend to find any movie that has a rating of ~7 or higher will generally be a very enjoyable film. I thought it would be interesting to understand further the factors that go into making a succesful movie, based on the criteria of rating. 



This project will go through the entire process of gathering, cleaning, manipulating, visualising, and statistically modeling data. 

```{r include=FALSE}
library(rvest)
library(tidyverse)
library(writexl)
library(lubridate)
library(xml2)
library(data.table)
library(ggpubr)
library(corrplot)
library(caret)
library(xgboost)
library(grid)
library(glmnet)
library(randomForest)
library(psych)
library(scales)
library(ggrepel)
library(rvest)


```


## Webscraping

To start, we need to collect data on as many relevent movies as we can. I thought the best way to use this was to create a webscraper that can search thorugh IMDb's developer mode pages to gather information on movies that fit a certain criteria, this criteria includes: 

* the movie must be English 
* have atleast 2500 IMDb votes 
* has to be a feature length movie 
* had to be created in 1970 or later.





#### Create inital dataframe that we will appened with our scraper function
```{r}
Movies_raw <- data.frame(matrix(ncol = 3,nrow = 0))
cols<- c("name","year","rating")
colnames(Movies_raw) <-cols
```

#### Main scraper function
```{r}
scraper <-function(n_movies) {
  page_number <- 1
  for (i in 1:as.integer(n_movies/50)) {
    link <- paste("https://www.imdb.com/search/title/?title_type=feature&release_date=1970-01-01,&num_votes=2000,&has=alternate-versions&certificates=US%3AG,US%3APG,US%3APG-13,US%3AR,US%3ANC-17&languages=en&sort=user_rating,desc&start=",page_number,"&ref_=adv_nxt",sep="")
    page <- read_html(link)
    
    name = page %>% html_nodes(".lister-item-header a") %>% html_text()

year = page %>%  html_nodes(".text-muted.unbold") %>%  html_text()

rating = page %>% html_nodes (".ratings-imdb-rating strong") %>%  html_text()
    
##get individual movie links:
movie_links = page %>% html_nodes(".lister-item-header a") %>%
  html_attr("href") %>% substr(., 1, 16) %>%
  paste("https://www.imdb.com", ., "/reference" , sep = "")


## function to get data from each of the generated movie links.

get_inner <- function(movie_link) {

  movie_page <- read_html(movie_link)
  cast = movie_page %>% 
    html_nodes(".itemprop .itemprop") %>% 
    html_text() %>%
    .[1:5] %>%
    paste(collapse = ",")
  
  synopsis <- movie_page %>%
    html_nodes(".titlereference-section-overview div:nth-child(1)") %>%
    html_text()
  
  genre1 <- movie_page %>% 
    html_nodes(".titlereference-header .ipl-inline-list__item:nth-child(3) a:nth-child(1)") %>% 
    html_text()
  
  genre2 <-movie_page %>% 
    html_nodes(".titlereference-header a+ a") %>% 
    html_text()
  
  imdb_votes <- movie_page %>% 
    html_nodes(".ipl-rating-star__total-votes") %>% 
    html_text()
  
  director <- movie_page %>% 
    html_nodes("hr+ .titlereference-overview-section a") %>% 
    html_text()
  
  awards <-  movie_page %>% 
    html_nodes(".titlereference-overview-section:nth-child(6) .ipl-inline-list__item:nth-child(1)") %>% 
    html_text() 
  
  budget <- movie_page %>% 
    html_nodes(".titlereference-section-box-office .ipl-zebra-list__item:nth-child(1) .ipl-zebra-list__label+ td") %>% 
    html_text() 
  
  opening_weekend <- movie_page %>% 
    html_nodes(".titlereference-section-box-office .ipl-zebra-list__item:nth-child(2) .ipl-zebra-list__label+ td") %>% 
    html_text()
  
  gross_world <- movie_page %>% 
    html_nodes(".titlereference-section-box-office .ipl-zebra-list__item~ .ipl-zebra-list__item+ .ipl-zebra-list__item .ipl-zebra-list__label+ td") %>% 
    html_text() 
  
  run_time <- movie_page %>% 
    html_nodes(".titlereference-section-additional-details .ipl-zebra-list__item:nth-child(2) .ipl-inline-list__item") %>% 
    html_text()
    
  return(c(genre1[1],genre2[1],cast,synopsis[1],imdb_votes[1],director[1],awards[1],
           budget[1],opening_weekend[1],gross_world[1],run_time[1]))
  
  
}
movie_inner_mat = sapply(movie_links, get_inner, USE.NAMES = FALSE)

movie_inner_df = as.data.frame(t(movie_inner_mat))

##clean movie_inner_df to account for movies with only one recorded genre.
# without this converting to a data frame wont work (records will be too long 
# since genre2 returns a vector)


  movie_inner_df$V2 <-sub("\n", NA, x = movie_inner_df$V2)

    
   Movies_raw_temp = data.frame(
  name = name,
  year = year,
  rating = rating,
  genre1 = movie_inner_df$V1,
  genre2 = movie_inner_df$V2,
  cast = movie_inner_df$V3,
  synopsis = movie_inner_df$V4,
  imdb_votes = movie_inner_df$V5,
  director = movie_inner_df$V6,
  awards = movie_inner_df$V7,
  budget = movie_inner_df$V8,
  opening_weekend = movie_inner_df$V9,
  gross_world = movie_inner_df$V10,
  run_time = movie_inner_df$V11,
  stringsAsFactors =
    FALSE
) 
    


    Movies_raw <- rbind(Movies_raw,Movies_raw_temp)
    
    page_number <-page_number+50
     ## percent complete tracker
     pct_complete <-paste(floor((page_number/n_movies)*100),"% complete")
    print(pct_complete)

  
  }
  return(Movies_raw)
}
#Movies_raw <- scraper(3400)
```

#### export Movies_raw to a csv.

```{r}
#write.csv(Movies_raw,file="Movies_raw.csv",row.names = FALSE)
```


## Data cleaning

In this section we take the raw data gather from web scraping and clean it up to create useable data that can be analysed.


```{r}
Movies_raw <- read.csv("Movies_raw.csv") 
```

first, create a copy to work on.

```{r}
Movies_Clean <- Movies_raw
```

Check for duplicate records

```{r}
tot.records <-nrow(Movies_Clean)
uniq.records <-nrow(unique(Movies_Clean))

paste("There are",tot.records,"total records and", uniq.records, "unique records.",tot.records-uniq.records,"duplicates found.")
```


remove duplicate records
```{r}
Movies_Clean <- unique(Movies_Clean)
```



clean the year variable
```{r}
## remove the parentheses in year and convert to a date format.
year_cleaner <- function(date) {
  date <- as.integer(gsub("[^0-9]","",date))
  
}
Movies_Clean$year <-year_cleaner(Movies_Clean$year)
  

```

clean the rating variable 
```{r}
Movies_Clean$rating <- as.numeric(Movies_Clean$rating)

```



#### generic function for removing commas and parentheses
```{r}

extra_char_cleaner <- function(x,as.numeric=FALSE) {
  x <- gsub("[(),a-b]","",x)
  if (as.numeric==TRUE){
    as.numeric(x)
  }

  }
```

cleaned the votes variable
```{r}
Movies_Clean$imdb_votes <- extra_char_cleaner(Movies_Clean$imdb_votes, 
                                              as.numeric =TRUE )
```

### cleaning budgets variable.


here we have budgets in a variety of currencies, i will convert them all to USD through use of  a web scraper. 


first we need to remove free space from the budget.

```{r}
Movies_Clean$budget <-str_trim(Movies_Clean$budget)

```
scrape all possible currency codes. then we can just see which of these exist in our data frame.

```{r}
currency_codes_link<-"https://www.iban.com/currency-codes"
currency_codes_page <- read_html(currency_codes_link)
currency_codes <- currency_codes_page %>% html_nodes("td:nth-child(3)") %>% html_text()

## clean currency_codes to get rid of the blank values

currency_codes <- currency_codes[nchar(currency_codes)==3]
```
lets take a look at the currency codes we have in our data frame, some of them will need converting to standardized currencies. 

```{r}
#codes_dirty includes some extras, just need to get the ones with 3 characters
codes_dirty <-gsub("[^A-Z.-]","",c(Movies_Clean$budget,Movies_Clean$opening_weekend,Movies_Clean$gross_world)) %>% 
  .[.!=""] %>% 
  .[!is.na(.)] %>% 
  unique(.) 

codes_clean <- codes_dirty[nchar(codes_dirty)==3]
```
remove none standardized currencies, we can add them back manually after.
```{r}
codes_removed <- codes_clean[!codes_clean %in% currency_codes]

codes_clean <- codes_clean[!codes_clean %in% codes_removed]
```

as we can see there are 17 different currency codes in the data frame split between the three columns; budget, opening_weekend, and gross_world.



#### currency code conversion rate data frame

lets create a dataframe with conversion rates that we can call later.



```{r}
# function for getting conversion rates from the currency codes.
currency_convertor<- function(currency_code) {
  link <- paste("https://www.xe.com/currencyconverter/convert/?Amount=1&From=",
                currency_code,"&To=USD",sep = "")
  page <- read_html(link)
  conversion_rate <- page %>% html_nodes(".iGrAod , .faded-digits") %>% html_text() 
  return(conversion_rate)
}
```

```{r}
##apply the currency convertor function
 conversion_rates <- sapply(codes_clean,currency_convertor)
 #transform output into a dataframe, and then convert to longer 
 #version
 conversion_rates_df <- as.data.frame(conversion_rates, stringsAsFactors =
                                        FALSE)
 conversion_rates_df <- as.data.frame(t(conversion_rates_df))
```

clean up for the conversion rate dataframe
```{r}
## add index
conversion_rates_df$currency_codes <- row.names(conversion_rates_df)
rownames(conversion_rates_df) <- 1:nrow(conversion_rates_df)
## remove V2 (useless info from scraping )
conversion_rates_df <- conversion_rates_df %>% 
  select(currency_codes, Conversion=V1) 
##manually add conversion (est)for the 5 depreciated currencies
conversion_rates_df <- 
  rbind(conversion_rates_df, data.frame(currency_codes=c("DEM","FRF","ITL","NLG","ESP"),
                                        Conversion=c("1.7293","0.172386","0.000581922", "0.513146","0.339349 ")))

## clean and change to numeric
conversion_rates_df$Conversion<- 
  as.numeric(gsub("[^0-9.-]", "", conversion_rates_df$Conversion))
```


```{r}
## function to get currency codes for other columns
##NOTE: should just change this to accept vectors.

get_conversion_rates <- function(col_name) {
## i should cut this part of the function down.
  #a lot isnt needed and its ugly.
  col_name_new <-"opening_weekend"
  col_name_new <- paste("Movies_Clean$",substr(col_name,1,4),
                    sep = "")
col_name_suffix <- paste(substr(col_name,1,4),"Converted")
col_name_new <- gsub("[:upper:]","",Movies_Clean[,col_name]) 
col_name_new <- str_trim(col_name_new)
col_name_new <- gsub("[^A-Z]","",col_name_new)
col_name_new[nchar(col_name_new) <3] <-NA
col_name_new <- substr(col_name_new,1,3)

#find cols with a currency code, match them to the conversion rate
## in the conversion_rates data frame.
for (i in 1:length(col_name_new)) {
  if  (col_name_new[i] %in% conversion_rates_df$currency_codes) {
    col_name_new[i] <- conversion_rates_df$Conversion[col_name_new[i]==conversion_rates_df$currency_codes]
  }}

## clean col to only get the value.
Movies_Clean[,col_name] <- str_trim(Movies_Clean[,col_name])
#remove everything after the currency.
Movies_Clean[,col_name] <-sub(" .*","",Movies_Clean[,col_name])
#remove all none numeric characters.
Movies_Clean[,col_name] <- gsub("[^0-9.-]","",Movies_Clean[,col_name])
#create new col with USD values.
Movies_Clean[,col_name_suffix] <-as.numeric(Movies_Clean[,col_name]) * 
  as.numeric(col_name_new)
## need to change NA values in new column for pre-exising
## values in the OG column.
for (i in 1:length(Movies_Clean[,col_name])) {
  if (is.na(Movies_Clean[,col_name_suffix][i])) {
    Movies_Clean[,col_name_suffix][i] <- Movies_Clean[,col_name][i]
  }
}
Movies_Clean$openning_USD
return(Movies_Clean[,col_name_suffix])
}           
```

apply the above function to the three money columns.
```{r}
## get cleaned budget in USD.
Movies_Clean$budget_USD <- as.integer(get_conversion_rates("budget"))

## get the cleaned opening_weekend in USD.
Movies_Clean$openning_USD <- as.integer(get_conversion_rates("opening_weekend"))
## get cleaned gross in USD
Movies_Clean$gross_USD <- as.integer(get_conversion_rates("gross_world"))
```

#### now we can adjust for inflation. 

note: this isnt a comprehensive adjustment since im adjusting the USD value not the orignal currency. For the sake of time and the scope of this project i dont think this will be a huge deal.

```{r}
## equation for inflation: CPI_today/CPI_year xusd_year =usd_today

## first we need to get CPI values for every year from 1970 to 2021. (2021 CPI are averaged from the final 2 quarters of 2020 and the first 2 of 2021.)

CPI_resource <- read_html("https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/")

tables <- CPI_resource %>% html_table(fill=TRUE)
view(tables)
CPI_df <- tables[[1]] %>% 
  select(X1,X14)
colnames(CPI_df) <-c("Year","Annual_CPI")

## remove the two top rows, these were the OG names from the website table. dont need them.
CPI_df <- CPI_df[3:nrow(CPI_df),]


```
#### add a multipier column to the CPI_df

i.e the amount the currency in a given year should be mulitplied by to account for inflation.

```{r}
## convert CPI to numeric.
CPI_df$Annual_CPI <-as.numeric(CPI_df$Annual_CPI)
for (i in 1:nrow(CPI_df)) {
  ## 277.948 is the estimated CPI for 2021.
  CPI_df$multiplier[i] <- 277.948/CPI_df$Annual_CPI[i]
}
```


#### function to apply inflation rate to our data
```{r}
apply_inflation <- function(x) {
  for (i in 1:nrow(Movies_Clean)) {
    x[i] <- x[i] * CPI_df$multiplier[CPI_df$Year ==Movies_Clean$year[i]]
  }
  return(x)
}
```




#### apply function to the 3 columns.
```{r}


Movies_Clean$budget_USD <- apply_inflation(Movies_Clean$budget_USD)

Movies_Clean$gross_USD <- apply_inflation(Movies_Clean$gross_USD)

Movies_Clean$openning_USD <-apply_inflation(Movies_Clean$openning_USD)
```

Finally, after taking a further look at the data there seems to be an issue with budget. for some of the movies that have less available information, the budget scraped from the web is actually the opening weekend or gross. To solve this i'm going to make all budgets NA if they don't have an opening weekend value associated with them. This isnt an ideal with to deal with the issue. However, due to the amount of data missing in these columns i doubt it will be an issue. I'm probably going to end up using these as factor variables, and it wont matter if some indiviudal recrods are slightly wrong.

```{r}
Movies_Clean$budget_USD[is.na(Movies_Clean$openning_USD)] <-NA
```


#### clean awards

starting with oscar wins.
```{r}
##clean awards columns, separate into Oscar wins, Oscar nominations
##and other nominations.
Movies_Clean$awards <- str_trim(Movies_Clean$awards)

## function to get just the number related to oscar wins
  
for (i in 1:nrow(Movies_Clean)) {
  if (grepl("Oscar.*",Movies_Clean$awards[i]) ==TRUE ) {
    Movies_Clean$Oscar_wins[i] <- gsub("Oscar.*","",Movies_Clean$awards[i])
  } else {
    Movies_Clean$Oscar_wins[i] <-0
  }
}
  for (i in 1:nrow(Movies_Clean)) {
    if(grepl("Nomin.*",Movies_Clean$Oscar_wins[i]) ==TRUE) {
      Movies_Clean$Oscar_wins[i] <- (gsub("[0-9^]","",
                                         Movies_Clean$Oscar_wins[i]))
    }
  }
## clean the returned string to just get the number
Movies_Clean$Oscar_wins[is.na(Movies_Clean$Oscar_wins)] <- 0
Movies_Clean$Oscar_wins <-as.integer(gsub("[a-z,A-Z]","",Movies_Clean$Oscar_wins))



```

oscar nominations
```{r}
######Oscar nominations
for (i in 1:nrow(Movies_Clean)) {
  if (grepl("Oscar.*",Movies_Clean$awards[i]) ==TRUE ) {
    Movies_Clean$Oscar_nominations[i] <- gsub("Oscar.*","",Movies_Clean$awards[i])
  } else {
    Movies_Clean$Oscar_nominations[i] <-0
  }
}
for (i in 1:nrow(Movies_Clean)) {
  if(grepl("Won.*",Movies_Clean$Oscar_nominations[i]) ==TRUE) {
    Movies_Clean$Oscar_nominations[i] <- (gsub("[0-9^]","",
                                        Movies_Clean$Oscar_nominations[i]))
  }
}
#extract the number of awards
Movies_Clean$Oscar_nominations <-gsub("[a-z,A-Z]","",Movies_Clean$Oscar_nominations)

## make blank records Na and convert to int
Movies_Clean$Oscar_nominations <- str_trim(Movies_Clean$Oscar_nominations)
Movies_Clean$Oscar_nominations <-as.integer(gsub("^$|^ $",0,Movies_Clean$Oscar_nominations))



```
other wins

```{r}
##other wins
## dont need to put this in  a for loop. do same as for other noms
for (i in 1:nrow(Movies_Clean)) {
  Movies_Clean$other_wins[i] <- gsub(" wins.* | .*Another ","",Movies_Clean$awards)[i]
  Movies_Clean$other_wins[i] <-(gsub("[^0-9]","",Movies_Clean$other_wins[i]))
  
}

## change blank values and NA to 0
Movies_Clean$other_wins <- gsub("^$|^ $",0,Movies_Clean$other_wins)
## convert to int
Movies_Clean$other_wins <- as.integer(Movies_Clean$other_wins)

```


other nominations
```{r}
##other nominations
Movies_Clean$other_nominations <- gsub(".*win","",Movies_Clean$awards)
Movies_Clean$other_nominations <- as.integer(gsub("[^0-9]","",
                                                  Movies_Clean$other_nominations))
## change NA values to 0. 
Movies_Clean$other_nominations[is.na(Movies_Clean$other_nominations)] <- 0

```
get rid of the NA values and replace with 0s.

```{r}
## some rows have no data for the awards. for these set all
## rewards columns to 0
Movies_Clean$Oscar_wins[is.na(Movies_Clean$Oscar_wins)] <-0
```

## trim synopsis
```{r}
Movies_Clean$synopsis <- str_trim(Movies_Clean$synopsis)
```

##clean run time

```{r}
Movies_Clean$run_time <- as.integer(gsub("[^0-9]","",
                                         Movies_Clean$run_time))
```
## clean genres, this mainly involes getting rid of the NAs. for this ill just set genre 2 equal to genre 1 when no genre 2 is provided.

```{r}
Movies_Clean$genre2[is.na(Movies_Clean$genre2)] <- Movies_Clean$genre1[is.na(Movies_Clean$genre2)]
```

## clean cast, this will involve seperating each of the 5 actors for each movie into their own column, this will make analysis much easier.

```{r}
## the first order of business is to split the current vector (length 1) into a vector of lenngth 5, i.e so each actor can be called individually, at the momeny its essentially a string stored as a vector.


Movies_Clean$cast <- strsplit(Movies_Clean$cast,",")

## now each of the 5 actors in a movie can be called individually, this means we can simply iterate through the data frame and assign each actor to its own column.

actor_split <- function(){
  for (actor in 1:5){
    col_name <- paste0("actor",actor)
    Movies_Clean[,col_name] <-NA
    for (i in 1:nrow(Movies_Clean)){
      
      Movies_Clean[,col_name][i] <- Movies_Clean$cast[[i]][actor]
    }
  }
  return(Movies_Clean)
}

Movies_Clean <-actor_split()



```


Remove old columns.

```{r}
Movies_Clean <- Movies_Clean %>% 
  select(-gross_world,-opening_weekend,
         -budget,-awards,-synopsis,-cast)
```
rearrange columns to be in a more intuitive order.

```{r}
Movies_Clean <-
  Movies_Clean %>% 
  select(name,year,director,actor1,actor2,actor3,actor4,actor5,imdb_votes,run_time,genre1,genre2,budget_USD,openning_USD,gross_USD,Oscar_wins,Oscar_nominations,other_wins,other_nominations,rating)
```

#### final checks to see if everything looks okay
```{r}
summary(Movies_Clean)
```

potential issues:
  * min IMDb_votes is 35, i set a filter to only get films with >2000 votes.
  * min run time is 23 minutes, there should only be feature length movies.
  *the minimum budget is $35.
  
  lets look at these one at a time.
  
```{r}
(Movies_Clean[Movies_Clean$imdb_votes==35,])

Movies_Clean <- Movies_Clean[-c(419),]
Movies_Clean[419,]
```
the Movie that only has 35 votes is also the movie thats only 23minutes long. its an epsiode of a series, not sure how it ended there but we can just remove it.


lets see if that has fixed the issue. now the minimum votes and minimum runtime make a lot more sense.
```{r}
summary(Movies_Clean)
```


the dataframe is good to go, with 3006 movies.

```{r}
write.csv(Movies_Clean,"Movies_Clean.csv",row.names = FALSE)
```


## EDA, Feature engineering, and modelling.

```{r}
Movies <- read.csv("Movies_Clean.csv")

## one of the Movies seems to have imported incorrectly. for now we're going to remove it.
summary(Movies)

```

## first we need to do a little clean up, it seems like there are some rows that havnt been properly formated. For now, since there are only 3 im just going to remove them.


```{r}
## remove rows that have incorrect formating
Movies <- Movies[c(-3366,-82,-339,-369),]


## set other wins NAs to 0.

Movies$other_wins[is.na(Movies$other_wins)] <-0

```



#### first lets split our data into a training and test set.




```{r}
set.seed(123)
pct <-0.8
df <- sample(nrow(Movies),nrow(Movies)*pct,replace=FALSE)

train <-Movies[df,]
test <-Movies[-df,]

paste("There are",nrow(train),"samples in the training data set, and",nrow(test),"in the test data set.   This is a ", pct*100,":",100-pct*100,
      "split.")
```

for feature engineering purposes, ill combine the two back together. But it is important to note that the data in the test data frame *will not* be used in any of the analysis, or in creating new features or predictive models.

```{r}
## First, we will store the rating scores for the test data to use at the end. we can then remove it, and combine it with the rest of the data to ensure we dont accidently use any of the training data for training purposes.
test_ratings <- test$rating

test$rating <-NA

all <- rbind(train,test)
glimpse(all)
```

lets do a quick check of na values

```{r}
nulcols <-all %>% 
  select(-rating) %>% 
  sapply(.,function(x) sum(is.na(x))) %>% 
  data.frame() %>% 
    rownames_to_column(var="Categories") 

colnames(nulcols) <- c( "Categories","Count")

nulcols %>% 
  filter(Count > 0)
```

There are only three columns with na, and its the three we expect.(excluding rating) 




### Exploratory data analysis

In this section we will explore the data a bit more, see how the predictor and outcome variables are related, and find areas that we can work on and improve in the feature engineering section.


To start lets see if our dependent variable is normally distributed, this is an essential part that will dictate whether or not our predictive models will work.
```{r}
ggdensity(all$rating,na.rm=TRUE)
```
As can been in the density plot, the data is fairly normal by it seems to be slightly skewed with a longer tail on the left. This can be seen more easily by reviewing a qqplot.


```{r}
ggqqplot(all$rating,)

skew(all$rating)
```
As could be seen, the data is slightly skewed to the left, this will of course have some effect on metrics that estimate location (i.e median and mean), although the skew is small enough for us to assume normal distribution and continue without transformation of the outcome variable. 



#### Lets take an initial look at how our numeric variables correlate to rating.


```{r}

## lets start of by getting just the numeric columns.
numeric_var_names <- which(sapply(all, is.numeric))

numeric_vars <- all[,numeric_var_names]

## find the correlation between all variables, which allows us to create a correlation matrix
cor_numeric_vars <-cor(numeric_vars,use="pairwise.complete.obs") ##pairwise.complete.obs used to take care of the NA values.


## now we can sort by rating, simply to give us an order so our correlation matrix is easier to read.
cor_sorted <- as.matrix(sort(cor_numeric_vars[,"rating"],decreasing = TRUE))

Cornames <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0)))

cor_numeric <-cor_numeric_vars[Cornames,Cornames]

corrplot.mixed(cor_numeric, tl.col="black", tl.pos = "lt")
```
It's clear from the above plot that none of the numeric predictors as they currently stand are particularly amazing. many of these will improve dramatically after some feature engineering. This correlation matrix also gives us a good initial indication as to whether or not we would run into multicolinearity issues, which can cause problems from some algorithms (e.g ones that are related to linear regression)


No two predictors correlate that well with each other, meaning none are redundant and therefore we shouldn't have issues with multiolinearity.




#### lets take a look at our outcome variable a little more, aswell as our current best numeric predictors. 

```{r}

all %>% 
  filter(!is.na(rating)) %>% 
  ggplot(aes(imdb_votes,rating)) +
  geom_point()

```

here we can see there is a hugely different magnitude between the smallest and largest values, taking the log of this predictor should increase correlation, but this is something that will be dealt with later.

#### IMDb votes exploration

lets see if year effects imdb_votes effect on rating (it could be possible that old movies i.e from the 70s would have less votes)

```{r}


VotesVTime <-all %>% 
  filter(!is.na(rating),) %>% 
  group_by(year) %>% 
  summarise(avg.imdb_votes = median(imdb_votes),mean.imdb_votes = mean(imdb_votes))

VotesVTime %>% 
  ggplot(aes(year,avg.imdb_votes,fill=avg.imdb_votes)) +
  geom_col() +
  theme_classic2() +
  theme(legend.position = 0) +
  scale_y_continuous(labels = comma) +
  labs(y="Median IMDb Votes", title="The change in average IMDb votes over time.") +
  geom_smooth(se=FALSE,colour="purple")+
    scale_fill_gradient() 
  
```

Clearly, older movies inherently have fewer votes irrelevent of their rating, giving older movies a disadvantage. This is something that should be addressed in the feature engineering section.



#### Non numeric variables

Before we start feature engineering lets see if we can learn anything for the non-numeric variables

#### director

first ill create a dataframe that shows the median rating of all directors Movies, aswell as the number of Movies they have directed in the training data set.
```{r}
director.df <-all %>% 
  filter(!is.na(rating)) %>% 
  group_by(director) %>% 
  summarise("Median_rating"=median(rating),count=n()) %>% 
  arrange(desc(Median_rating)) %>% 
  filter(count >2)



  
head(director.df)
```

An issue arises, a bunch of directors only have 1 data point in the training data set.We will have to take this into account when creating our features. 

Despite this, the director very clearly, and rather intuitively effects a movies rating substantially 


```{r}
## median rating variable
median.rating <- all %>% 
  filter(!is.na(rating)) %>% 
  summarise(median(rating)) %>% 
  .[1,1]

director.df %>% 
  ggplot(aes(reorder(director,Median_rating),Median_rating)) +
  geom_point() +
  geom_hline(yintercept = median.rating, colour="red",linetype="dashed") +
  theme_bw()+
  geom_text(aes(x=25,y=6.5),label=paste("Median rating:",median.rating),colour="black",check_overlap = T) +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "The median rating of each Director.",
       subtitle = "Names have not been included to remove clutter.")
```

## Feature Engineering


In this section we will go through our predictor variables one by one and see how they can be manipulated, combined, or used to create new variables to improve predictability of our final model. 

#### imdb_votes


As was seen in the previous section year greatly influences IMDb votes, resulting in votes not being a very viable predictor, intuitively this seems odd, you would think a movie with more votes would see higher ratings. We have to account for the fact that IMDb didnt exist for a large portion of these movies, and newer, younger audiences are probably more likely to be involved with a website like this.


To start, we will see how creating a new variable that looks at imdb votes *relative* to those in the same year effects its predictive power.

```{r}
for (i in 1:nrow(all)) {
  all$votes.rel[i] <- all$imdb_votes[i]/ VotesVTime$avg.imdb_votes[VotesVTime$year ==all$year[i]] 
}
```

lets see how our new feature correlates with rating.

```{r}
all %>% 
  filter(!is.na(rating)) %>% 
  ggplot(aes(log(votes.rel),rating)) +
  geom_point()

cor(all$rating,log(all$votes.rel),use="pairwise.complete.obs")


```




#### Director


A reasonable way of improving this particular variable is to convert it to ordinal data, where directors are categorized based on their average Movie rating. If a director isn't in the training data ill just put them in the central group. (note: to improve this, it could be worth looking at the most common genre of movie these directors make and use that to choose what group they go in. i.e horror Movie directors will typically have lower ratings than drama directors.)

```{r}

## the quantiles we want to use to split the groups
director.quantiles <- c(0.05,0.15,0.35,0.65,0.75,0.85,0.95)

## setting the value of the highest ranked group
director.df$director_rating <- length(director.quantiles) +1

## setting the value for all subsequent categories

for (i in 1:length(director.quantiles)) {
  director.df$director_rating[!director.df$director %in% head(director.df$director, floor(nrow(director.df))*director.quantiles[i])] <-length(director.quantiles)+1 - i
}

```

Now to create a column in all that corresponds to these ratings.

```{r}
for (i in 1:nrow(all)) {
  if (all$director[i] %in% director.df$director==TRUE){
    all$director_rating[i] <- director.df$director_rating[all$director[i] == director.df$director]
  } else {
    all$director_rating[i] <-4
  }
  
}
```

lets see how good it is and see if it needs changing.

```{r}
cor(all$rating,all$director_rating,use="pairwise.complete.obs")
```
we have to take into consideration that many of the directors on here only have 1 movie, that isnt enough information to categories them correctly, it may be better to simply put these in the middle group.

```{r}
all %>% 
  filter(!is.na(rating)) %>% 
  ggplot(aes(director_rating,rating)) +
  stat_summary(fun="median",geom = "bar") 


cor(all$rating,all$director_rating,use="pairwise.complete.obs")
```


#### actors

I think it makes sense to treat actors in much the same way as directors, of course here we have 5 actor columns, ill simply do the same as above for each of the 5 and average out the resultant value. It may be worth applying a weight to actors that are listed earlier (i.e the top credited actor), but we can experiment and see if that's needed.



We need to start by assigning each actor to a category
```{r}
##A function to assign a score to each actor in the dataframe (including actors from all 5 columns.)

actor.score <- function() {
  actor.df <-data.frame() 
  for (i in 1:5){                 ##iterate through the 5 actor columns
    col_name <- paste0("actor",i) 
    med_rating <- all %>%  ##get the medium rating for actors in a column
      filter(!is.na(rating)) %>% 
      group_by(all %>% filter(!is.na(rating)) %>% .[,col_name]) %>% 
      summarise(med.rating=median(rating))
    actor.df <- rbind(actor.df,med_rating) ## put all the results into a dataframe
    
    
  }
  names(actor.df)[1]<-"Actor"
  
  actor.df <- actor.df %>%   ## get median rating for each unique actor.
    group_by(Actor) %>% 
    summarise(rating =median(med.rating),count = n()) %>% 
    arrange(desc(rating)) %>% 
    filter(count>2) ### we only want actors that are in 3 or more movies. We cant use a single movie to predict the scores of other movies.
  
  return(actor.df)
  
}

actor.df <- actor.score()
```

Now we need to apply the actor scores to actors in our data frame, and assign and overall score for each movie based on the 5 top credited actors. 



```{r}



all$actor_score <-0
for (i in 1:5) {
  col_name <- paste0("actor",i)
  for (j in 1:nrow(all)) {
    if(all[,col_name][j] %in% actor.df$Actor) {
      all$actor_score[j] <- all$actor_score[j] +actor.df$rating[actor.df$Actor ==all[,col_name][j]]
    } else{
      all$actor_score[j] <-all$actor_score[j] +median(actor.df$rating)
    }
  }
}

```
There are a bunch of actors in the test set that arnt present in the training set. This means they dont have an associated value with them. For now, i have assigned a central value to these actors. *I will need to come back and find an appropriate way to assign values to these. for the time being though it will have to do.*






```{r}
cor(all$actor_score,all$rating,use = "pairwise.complete.obs")
```

as we can see the correlation in the training data is very high. in reality this is just because it is over fit to the training data. this is something i will have to comeback to and address. cross validation may be needed.


#### awards

as it stands the awards variables dont offer much in terms of predictive power, i think we can solve that by combining the three variables, applying different weights to each caregory.

```{r}
all$award_score <- all$Oscar_wins +all$Oscar_nominations +all$other_wins*0.05 +
  all$other_nominations*0.05

cor(all$rating,all$award_score,use = "pairwise.complete.obs")

```

#### genre

```{r}


median_ratings_genre1 <-all %>% 
  filter(!is.na(rating)) %>% 
  group_by(genre1) %>% 
  summarise(median_rating = median(rating),count=n()) %>% 
  arrange(desc(median_rating)) 

```

```{r}
median_ratings_genre2 <-all %>% 
  filter(!is.na(rating)) %>% 
  group_by(genre2) %>% 
  summarise(median_rating = median(rating),count=n()) %>% 
  arrange(desc(median_rating)) 

```

Bother genre1 and genre 2 seem to have an effect on rating, we may however see a better result if we combine the two together

```{r}

all$genres <- paste(all$genre1, all$genre2)

median_ratings_genres <-all %>% 
  filter(!is.na(rating)) %>% 
  group_by(genres) %>% 
  summarise(median_rating = median(rating),count=n()) %>% 
  arrange(desc(median_rating)) 

median_ratings_genres %>% 
  ggplot(aes(reorder(genres,median_rating),median_rating)) +
  geom_point() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  labs(x="Genres",
       title="Genres affect on rating",
       subtitle = "the extremes are labelled with genre name, and how frequent they appear in the database") +
  geom_text_repel(aes(label=ifelse(median_rating >7.7 |median_rating <4.5,
                                   paste(genres,
                                         count)
                                   ,""))) 
  
## set them to be factor variables (for now, i may change this.)

all$genre1 <- as.factor(all$genre1)
all$genre2 <- as.factor(all$genre2)


```

again, there's a clear trend here and it seems that some genres are for sure better in terms of getting a higher rating, however, many of the categoires only have a few data points in them. Because of this, it would be better to more broadly categories them, or remove genres that have few data points when creating dummy variables. 


### opening, budget, and gross

```{r}
nulcols <-all %>% 
  select(-rating,gross_USD,budget_USD,openning_USD) %>% 
  sapply(.,function(x) sum(is.na(x))) %>% 
  data.frame() %>% 
    rownames_to_column(var="Categories") 

colnames(nulcols) <- c( "Categories","NA_Count")

nulcols %>% 
  filter(NA_Count > 0)
```
As we can see, over half of all the records have an NA value in atleast one of the three columns, because of this itll be hard to use these variables without going back and filling in the missing data.


For now im going to use them to create a new factor variable, with the idea that Movies that have a recorded budget, oppenning, and gross are more likely to have been succesful movies, and thus higher rated.


```{r}
all$Money.present <- as.factor(ifelse(is.na(all$gross_USD +all$budget_USD +all$openning_USD ), 0,1))
```

```{r}
all %>% 
  filter(!is.na(rating)) %>% 
  ggplot(aes(Money.present,rating)) +
  geom_boxplot()
  
```
It would appear that movies that have data for all 3 variables do, on average score a higher rating. although the difference seems rather small and there's quite large variance, with several outlier points. 


#### Random forst for identifying variable importance.

First we need to clean up the dataframe and remove unwanted/un-used variables.

```{r}
all <- all %>% select(c(-actor1,-actor2,-actor3,-actor4,-actor5,-budget_USD,
                        -openning_USD,-gross_USD,-Oscar_wins,-Oscar_nominations,-other_wins,
                        -other_nominations,-name,-director,-genres,-year))
```


since we're just using this for variable importance we can keep it simple, no need for cross validation or optimization. 


```{r}

RF_vars <-randomForest(x=all[!is.na(all$rating),-5],
             y=all$rating[!is.na(all$rating)],
             ntree=200,
             importance=TRUE)

varImpPlot(RF_vars,type = 1)
```

This essentially shows the effect of random permutation of each variable, which removes that variables predictive power, if it results in a higher MSE, then that variable has a large effect on the model and is more important of a variable. 



#### Pre-processing the data.



We're going to be utalising KNN and other algorithms that are either distance based methods or gradient descent methods, both of these require standardization and normalization of numeric variables. we also need to create dummy variables from our categorical data.

```{r}
numericvars <- which(sapply(all,is.numeric))

numericvars.df <- all[,numericvars] %>% select(-rating)

factors.df <- all[,!names(all) %in% names(numericvars.df)] %>% select(-rating)
```


#### Skewness

for our models to work we must assume our variables are normally distributed. an easy way to see if this assumption is true is by looking at the skewness of each variable, typically we look for a value between -0.8 and 0.8 for the assumption to be true. for values out of this range we will take the log of them which should result in a more Gaussian distribution 

```{r}
for (i in 1:ncol(numericvars.df)) {
  if (abs(skew(numericvars.df[,i]) >0.8)){
    numericvars.df[,i] <- log(numericvars.df[,i] +1) 
  }
}
```

#### Normalizing the data

```{r}
PreProc <- preProcess(numericvars.df,method=c("center","scale"))
print(PreProc)
```

```{r}
norm.DF <- predict(PreProc,numericvars.df)
```


#### One hot encoding of the factor variables.


```{r}
dummies.df <- as.data.frame(model.matrix(~.-1 ,factors.df))
```

note: we use the -1 in the formula so we dont have an intercept value, or more accurately the intercept term is given the name its based on (since we want one hot encoding we dont need an intercept value, this would only be needed for linear regression where we would run into redundancy issues that lead to multicolinearity .)

### cleaning up the dummy variables by removing those that either: are not present in the test data, or have fewer than 10 ones in the train data



```{r}
emptylevels.test <- which(colSums(dummies.df[(nrow(all[!is.na(all$rating),])+1):nrow(all),])==0)

## remove these levels from dummies.df

dummies.df <-dummies.df[,-emptylevels.test]


```

Now to remove levels that are either not present, or are rarely present in the training data.

```{r}
emptylevels.train <- which(colSums(dummies.df[1:nrow(all[!is.na(all$rating),]),])<10)

dummies.df <-dummies.df[,-emptylevels.train]
```

now we need to combine our numerics and dummy dataframes.

```{r}
all1 <- cbind(norm.DF,dummies.df)
```




#### knn regression predictor.

Finally i'll use a basic knn model to introduce local information to the model and create one last predictor. 


Here we're going to be using the train function from caret to perform cross validation and find the ideal number for k.

```{r}

set.seed(123)

train1 <- all1[!is.na(all$rating),]
train1$rating <- train$rating
test1 <- all1[is.na(all$rating),]

## for knn we need to seperate the outcome variable form the predictor variables

train.knn.x <-train1 %>% select(-rating)
train.knn.y <- train$rating
test.knn.x <- test1

control <-trainControl(
  method="cv",
  number =10
)

knn.model <- train(x=train.knn.x,
                   y=train.knn.y,
                   method="knn",
                   trcontrol=control,
                   tuneLength=30)
best.k <- knn.model$results$k[knn.model$results$RMSE==min(knn.model$results$RMSE)] ## select the best k value from the cross validation test.


plot(knn.model)
```

here were using the best k value found in cross validation to create our actual model.
```{r}
knnreg.fit <-knnreg(x=train.knn.x,
                    y=train.knn.y,
                   method="knn",
                   k=best.k)
knn.pred.train <- predict(knnreg.fit,train.knn.x)
mean((knn.pred.train-all$rating[!is.na(all$rating)])^2)

knn.pred.test <- predict(knnreg.fit,test.knn.x)
knn.pred <- append(knn.pred.train,knn.pred.test)

all1$knn.pred <- knn.pred                  
```

now that we have a new predictor we need to ensure we preprocess the data once more.

```{r}

preproc <- preProcess(all1[,c(colnames(numericvars.df),"knn.pred")],
                      method = c("center","scale"))

all1[,c(colnames(numericvars.df),"knn.pred")] <- predict(preproc,all1[,c(colnames(numericvars.df),"knn.pred")])
```






```{r}
##  update our train and test data sets to have the knn predictor variable.

train1 <-all1[!is.na(all$rating),]

train1$rating <- all$rating[!is.na(all$rating)]

test1 <-all1[rownames(test),]
```


## modelling 


elastic net

```{r}
set.seed(5435)

rownames <- sample(nrow(train1),nrow(train1)*pct,replace=FALSE)


train.net.x <- as.matrix(train1[rownames,] %>% select(-rating))
train.net.y <- train1$rating[rownames]

holdout.net.x <- as.matrix(train1[-rownames,] %>% select(-rating))
holdout.net.y <- train1$rating[-rownames]

```

```{r}
set.seed(53)

alpha.values <- seq(0,1,0.1)
alpha.fits <- data.frame()
for (i in 1:length(alpha.values)) {
  temp.fit <-cv.glmnet(train.net.x,train.net.y,type.measure = "mse",
                        alpha=alpha.values[i],family="gaussian",nfolds = 50)
  temp.pred <-predict(temp.fit,newx=holdout.net.x,s=temp.fit$lambda.1se)
  temp.df <- data.frame(alpha=(i-1)/10,
                        MSE=mean((holdout.net.y-temp.pred)^2))
  alpha.fits <- rbind(alpha.fits,temp.df)
  
}
best.alpha <-alpha.fits$alpha[alpha.fits$MSE==min(alpha.fits$MSE)]

alpha.fits

```
as we can see from the above tests, an alpha value of 0.5 provides the best results. We will use this for our actual model.

```{r}
## creating test matrices based on our test1 set (this is completely unseen/new data data)
set.seed(8312)

test.x <- as.matrix(test1)
test.y <- test_ratings


knn.fit <- cv.glmnet(train.net.x,train.net.y,type.measure = "mse",
                        alpha=best.alpha,family="gaussian")

elastic.prediction <- predict(knn.fit,s=knn.fit$lambda.min,newx=test.x)


mean((test.y-elastic.prediction)^2) ## output the MSE 


```


xgboosted tree model 

First we need to tune the hyperparamters, of which there are a few. An efficient  method to go about this is by utalising expand.grid to generate a bunch of sets of hyperparameters. 
```{r}
xgb_grid <- expand.grid(
nrounds =500 ,
eta = c(0.1, 0.05, 0.01),
max_depth = c(2, 3, 4, 5, 6),
gamma = 0,
colsample_bytree=1,
min_child_weight=c(1, 2, 3, 4 ,5),
subsample=1
)
```

now we can simply use our xgb_grid in caret's built in xgb function to perform cross validation and find the optimal set of parameters. 

```{r}
set.seed(7123)
xgb_control <- trainControl(method = "cv",number=3)

xgb_fit <- train(x=as.matrix(train1 %>% select(-rating,-knn.pred)) , y=train1$rating, method='xgbTree', trControl= xgb_control, tuneGrid=xgb_grid) 
xgb_fit$bestTune
```

Now that we have ideal hyperparameters we can work with the xgboost package to create the acutal model.

```{r}
##xgboost requires the x variables to be in a xgb.DMatrix
xgb.label <- train1$rating
xgb.train <- xgb.DMatrix(data =as.matrix(train1 %>% select(-rating,-knn.pred)) ,label=xgb.label)
xgb.test <- xgb.DMatrix(data=as.matrix(test1 %>% select(-knn.pred)))
```


```{r}
xgb_param<-list(
        objective = "reg:squarederror",
        booster = "gbtree",
        eta=0.05, 
        gamma=0,
        max_depth=3, 
        min_child_weight=1, 
        subsample=1,
        colsample_bytree=1
)
```

Now to perform cross validation to determine how many rounds is best.

```{r}
set.seed(332145)
xgb_cv <- xgb.cv(params =xgb_param,data=xgb.train,nrounds = 2000,nfold = 5,showsd = T,stratified = T,print_every_n = 20,early_stopping_rounds = 10,maximize = F )
```
ideal number of rounds seems to be 1045.

```{r}
set.seed(433245)
xgb.model <- xgb.train(params=xgb_param,data=xgb.train,nround=236)
```

```{r}

xgb.pred <- predict(xgb.model,xgb.test)

mean((test.y-xgb.pred)^2) ## output the MSE
```


```{r}
library(Ckmeans.1d.dp) #required for ggplot clustering
mat <- xgb.importance (feature_names = colnames(train1 %>% select(-rating, -knn.pred)),model = xgb.model)
xgb.ggplot.importance(importance_matrix = mat[1:20], rel_to_first = T)
```

